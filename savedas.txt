#!/usr/bin/env python3
"""
Advanced NTFS Security Descriptor Analyzer
Enterprise-grade forensic tool for comprehensive NTFS security analysis using libfsntfs

Features:
- Complete MFT analysis with attribute parsing
- Full $Secure system file analysis
- Security descriptor parsing with ACL decoding  
- Multiple output formats (JSON, XML, raw hex, detailed text)
- Comprehensive error handling and logging
- Performance optimizations for large volumes
- Integration with NTFS metadata structures
"""

import pyfsntfs
import struct
import sys
import argparse
import json
import xml.etree.ElementTree as ET
import logging
import hashlib
import time
from datetime import datetime
from typing import Optional, List, Tuple, Dict, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import concurrent.futures
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ntfs_analyzer.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class NTFSVolumeInfo:
    """NTFS Volume information"""
    total_clusters: int = 0
    cluster_size: int = 0
    mft_size: int = 0
    volume_size: int = 0
    serial_number: str = ""
    version: str = ""
    
@dataclass 
class SecurityIdentifier:
    """Windows Security Identifier (SID) representation"""
    revision: int = 0
    authority: int = 0
    sub_authorities: List[int] = None
    raw_data: bytes = b''
    
    def __post_init__(self):
        if self.sub_authorities is None:
            self.sub_authorities = []
    
    @classmethod
    def from_bytes(cls, data: bytes) -> Optional['SecurityIdentifier']:
        """Parse SID from binary data"""
        if len(data) < 8:
            return None
        
        try:
            revision = data[0]
            sub_auth_count = data[1]
            authority = struct.unpack('>Q', b'\x00\x00' + data[2:8])[0]
            
            if len(data) < 8 + (sub_auth_count * 4):
                return None
                
            sub_authorities = []
            for i in range(sub_auth_count):
                sub_auth = struct.unpack('<I', data[8 + i*4:12 + i*4])[0]
                sub_authorities.append(sub_auth)
            
            return cls(revision, authority, sub_authorities, data)
        except:
            return None
    
    def to_string(self) -> str:
        """Convert SID to string format"""
        if not self.sub_authorities:
            return f"S-{self.revision}-{self.authority}"
        return f"S-{self.revision}-{self.authority}-{'-'.join(map(str, self.sub_authorities))}"

class AccessControlEntry:
    """ACE (Access Control Entry) parser"""
    
    ACE_TYPES = {
        0x00: "ACCESS_ALLOWED_ACE",
        0x01: "ACCESS_DENIED_ACE", 
        0x02: "SYSTEM_AUDIT_ACE",
        0x03: "SYSTEM_ALARM_ACE",
        0x04: "ACCESS_ALLOWED_COMPOUND_ACE",
        0x05: "ACCESS_ALLOWED_OBJECT_ACE",
        0x06: "ACCESS_DENIED_OBJECT_ACE",
        0x07: "SYSTEM_AUDIT_OBJECT_ACE",
        0x08: "SYSTEM_ALARM_OBJECT_ACE",
        0x09: "ACCESS_ALLOWED_CALLBACK_ACE",
        0x0A: "ACCESS_DENIED_CALLBACK_ACE",
        0x0B: "ACCESS_ALLOWED_CALLBACK_OBJECT_ACE",
        0x0C: "ACCESS_DENIED_CALLBACK_OBJECT_ACE",
        0x0D: "SYSTEM_AUDIT_CALLBACK_ACE",
        0x0E: "SYSTEM_ALARM_CALLBACK_ACE",
        0x0F: "SYSTEM_AUDIT_CALLBACK_OBJECT_ACE",
        0x10: "SYSTEM_ALARM_CALLBACK_OBJECT_ACE",
        0x11: "SYSTEM_MANDATORY_LABEL_ACE",
        0x12: "SYSTEM_RESOURCE_ATTRIBUTE_ACE",
        0x13: "SYSTEM_SCOPED_POLICY_ID_ACE"
    }
    
    ACE_FLAGS = {
        0x01: "OBJECT_INHERIT_ACE",
        0x02: "CONTAINER_INHERIT_ACE", 
        0x04: "NO_PROPAGATE_INHERIT_ACE",
        0x08: "INHERIT_ONLY_ACE",
        0x10: "INHERITED_ACE",
        0x20: "SUCCESSFUL_ACCESS_ACE_FLAG",
        0x40: "FAILED_ACCESS_ACE_FLAG"
    }
    
    def __init__(self, data: bytes, offset: int = 0):
        self.raw_data = data
        self.offset = offset
        self.parse()
    
    def parse(self):
        """Parse ACE structure"""
        if len(self.raw_data) < 8:
            raise ValueError("ACE too short")
        
        header = struct.unpack('<BBH', self.raw_data[:4])
        self.ace_type = header[0] 
        self.ace_flags = header[1]
        self.ace_size = header[2]
        
        if len(self.raw_data) < self.ace_size:
            raise ValueError("ACE data truncated")
        
        # Parse access mask
        self.access_mask = struct.unpack('<I', self.raw_data[4:8])[0]
        
        # Parse SID (starts at offset 8 for standard ACEs)
        if self.ace_size > 8:
            self.sid = SecurityIdentifier.from_bytes(self.raw_data[8:self.ace_size])
        else:
            self.sid = None
    
    def get_type_string(self) -> str:
        return self.ACE_TYPES.get(self.ace_type, f"UNKNOWN_ACE_TYPE_{self.ace_type:02x}")
    
    def get_flags_list(self) -> List[str]:
        flags = []
        for bit, name in self.ACE_FLAGS.items():
            if self.ace_flags & bit:
                flags.append(name)
        return flags
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'type': self.get_type_string(),
            'type_code': self.ace_type,
            'flags': self.get_flags_list(),
            'flags_code': self.ace_flags,
            'size': self.ace_size,
            'access_mask': f"0x{self.access_mask:08x}",
            'sid': self.sid.to_string() if self.sid else None,
            'raw_hex': self.raw_data[:self.ace_size].hex()
        }

class AccessControlList:
    """ACL (Access Control List) parser"""
    
    def __init__(self, data: bytes):
        self.raw_data = data
        self.aces = []
        self.parse()
    
    def parse(self):
        """Parse ACL structure"""
        if len(self.raw_data) < 8:
            raise ValueError("ACL too short")
        
        header = struct.unpack('<BBHI', self.raw_data[:8])
        self.revision = header[0]
        self.sbz1 = header[1] 
        self.acl_size = header[2]
        self.ace_count = header[3]
        
        if len(self.raw_data) < self.acl_size:
            raise ValueError("ACL data truncated")
        
        # Parse ACEs
        offset = 8
        for i in range(self.ace_count):
            if offset >= self.acl_size:
                break
            try:
                ace = AccessControlEntry(self.raw_data[offset:], offset)
                self.aces.append(ace)
                offset += ace.ace_size
                # Align to 4-byte boundary
                offset = (offset + 3) & ~3
            except Exception as e:
                logger.warning(f"Failed to parse ACE {i}: {e}")
                break
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'revision': self.revision,
            'size': self.acl_size,
            'ace_count': self.ace_count,
            'aces': [ace.to_dict() for ace in self.aces]
        }

class EnhancedSecurityDescriptor:
    """Enhanced Security Descriptor with full ACL parsing"""
    
    CONTROL_FLAGS = {
        0x0001: "SE_OWNER_DEFAULTED",
        0x0002: "SE_GROUP_DEFAULTED",
        0x0004: "SE_DACL_PRESENT", 
        0x0008: "SE_DACL_DEFAULTED",
        0x0010: "SE_SACL_PRESENT",
        0x0020: "SE_SACL_DEFAULTED",
        0x0040: "SE_DACL_TRUSTED",
        0x0080: "SE_SERVER_SECURITY",
        0x0100: "SE_DACL_AUTO_INHERIT_REQ",
        0x0200: "SE_SACL_AUTO_INHERIT_REQ", 
        0x0400: "SE_DACL_AUTO_INHERITED",
        0x0800: "SE_SACL_AUTO_INHERITED",
        0x1000: "SE_DACL_PROTECTED",
        0x2000: "SE_SACL_PROTECTED",
        0x4000: "SE_RM_CONTROL_VALID",
        0x8000: "SE_SELF_RELATIVE"
    }
    
    def __init__(self, data: bytes):
        self.raw_data = data
        self.owner_sid = None
        self.group_sid = None
        self.dacl = None
        self.sacl = None
        self.parse_header()
        self.parse_components()
    
    def parse_header(self):
        """Parse security descriptor header"""
        if len(self.raw_data) < 20:
            raise ValueError("Security descriptor too short")
        
        header = struct.unpack('<BBHHIIII', self.raw_data[:20])
        self.revision = header[0]
        self.sbz1 = header[1]
        self.control = header[2]
        self.offset_owner = header[3]
        self.offset_group = header[4]
        self.offset_sacl = header[5] 
        self.offset_dacl = header[6]
    
    def parse_components(self):
        """Parse SIDs and ACLs"""
        try:
            # Parse Owner SID
            if self.offset_owner and self.offset_owner < len(self.raw_data):
                self.owner_sid = SecurityIdentifier.from_bytes(self.raw_data[self.offset_owner:])
            
            # Parse Group SID
            if self.offset_group and self.offset_group < len(self.raw_data):
                self.group_sid = SecurityIdentifier.from_bytes(self.raw_data[self.offset_group:])
            
            # Parse DACL
            if self.offset_dacl and self.offset_dacl < len(self.raw_data):
                self.dacl = AccessControlList(self.raw_data[self.offset_dacl:])
            
            # Parse SACL
            if self.offset_sacl and self.offset_sacl < len(self.raw_data):
                self.sacl = AccessControlList(self.raw_data[self.offset_sacl:])
                
        except Exception as e:
            logger.warning(f"Error parsing security descriptor components: {e}")
    
    def get_control_flags(self) -> List[str]:
        """Decode control flags"""
        flags = []
        for bit, name in self.CONTROL_FLAGS.items():
            if self.control & bit:
                flags.append(name)
        return flags
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'revision': self.revision,
            'control': {
                'value': f"0x{self.control:04x}",
                'flags': self.get_control_flags()
            },
            'owner_sid': self.owner_sid.to_string() if self.owner_sid else None,
            'group_sid': self.group_sid.to_string() if self.group_sid else None,
            'dacl': self.dacl.to_dict() if self.dacl else None,
            'sacl': self.sacl.to_dict() if self.sacl else None,
            'offsets': {
                'owner': self.offset_owner,
                'group': self.offset_group,
                'dacl': self.offset_dacl,
                'sacl': self.offset_sacl
            },
            'size': len(self.raw_data),
            'raw_hex': self.raw_data.hex()
        }

class EnhancedSDSEntry:
    """Enhanced SDS Entry with full parsing"""
    
    def __init__(self, data: bytes, offset: int):
        self.offset = offset
        self.raw_data = data
        self.security_descriptor = None
        self.parse_header()
        self.parse_security_descriptor()
    
    def parse_header(self):
        """Parse SDS entry header"""
        if len(self.raw_data) < 20:
            raise ValueError("SDS entry too short")
        
        header = struct.unpack('<IIIII', self.raw_data[:20])
        self.hash = header[0]
        self.security_id = header[1]
        self.sd_offset = header[2]
        self.sd_size = header[3]
        self.reserved = header[4]
        
        # Calculate hash for verification
        if self.sd_size > 0 and len(self.raw_data) >= 20 + self.sd_size:
            sd_data = self.raw_data[20:20 + self.sd_size]
            calculated_hash = self.calculate_security_hash(sd_data)
            self.hash_valid = (calculated_hash == self.hash)
        else:
            self.hash_valid = False
    
    def calculate_security_hash(self, data: bytes) -> int:
        """Calculate NTFS security hash"""
        hash_val = 0
        for i in range(0, len(data), 4):
            chunk = data[i:i+4]
            if len(chunk) == 4:
                hash_val += struct.unpack('<I', chunk)[0]
            else:
                # Handle remaining bytes
                padded = chunk + b'\x00' * (4 - len(chunk))
                hash_val += struct.unpack('<I', padded)[0]
        return hash_val & 0xFFFFFFFF
    
    def parse_security_descriptor(self):
        """Parse embedded security descriptor"""
        if self.sd_size > 0 and len(self.raw_data) >= 20 + self.sd_size:
            try:
                sd_data = self.raw_data[20:20 + self.sd_size]
                self.security_descriptor = EnhancedSecurityDescriptor(sd_data)
            except Exception as e:
                logger.warning(f"Failed to parse security descriptor at offset 0x{self.offset:x}: {e}")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'offset': f"0x{self.offset:x}",
            'hash': {
                'value': f"0x{self.hash:08x}",
                'valid': self.hash_valid
            },
            'security_id': self.security_id,
            'sd_offset': self.sd_offset,
            'sd_size': self.sd_size,
            'reserved': f"0x{self.reserved:08x}",
            'security_descriptor': self.security_descriptor.to_dict() if self.security_descriptor else None
        }

class AdvancedNTFSAnalyzer:
    """Advanced NTFS analyzer with comprehensive capabilities"""
    
    def __init__(self, image_path: str):
        self.image_path = image_path
        self.fs = None
        self.secure_file = None
        self.sds_stream = None
        self.volume_info = NTFSVolumeInfo()
        self.mft_entries = {}
        self.security_cache = {}
        
    def open(self):
        """Open NTFS image and initialize structures"""
        try:
            logger.info(f"Opening NTFS image: {self.image_path}")
            self.fs = pyfsntfs.file()
            self.fs.open(self.image_path)
            
            # Get volume information
            self.gather_volume_info()
            
            # Initialize $Secure analysis
            self.initialize_secure_analysis()
            
            logger.info("NTFS image opened successfully")
            
        except Exception as e:
            logger.error(f"Error opening NTFS image: {e}")
            raise
    
    def gather_volume_info(self):
        """Gather comprehensive volume information"""
        try:
            # Get basic volume info
            self.volume_info.cluster_size = self.fs.get_cluster_size()
            self.volume_info.total_clusters = self.fs.get_number_of_clusters()
            self.volume_info.volume_size = self.volume_info.cluster_size * self.volume_info.total_clusters
            
            # Get MFT info
            mft_file = self.fs.get_file_by_path("/$MFT")
            if mft_file:
                self.volume_info.mft_size = mft_file.get_size()
            
            # Get volume serial and version if available
            try:
                volume_info = self.fs.get_volume_information()
                if hasattr(volume_info, 'serial_number'):
                    self.volume_info.serial_number = f"0x{volume_info.serial_number:08x}"
                if hasattr(volume_info, 'version'):
                    self.volume_info.version = volume_info.version
            except:
                pass
                
            logger.info(f"Volume size: {self.volume_info.volume_size:,} bytes")
            logger.info(f"Cluster size: {self.volume_info.cluster_size:,} bytes")
            logger.info(f"MFT size: {self.volume_info.mft_size:,} bytes")
            
        except Exception as e:
            logger.warning(f"Could not gather complete volume info: {e}")
    
    def initialize_secure_analysis(self):
        """Initialize $Secure file analysis"""
        try:
            self.secure_file = self.fs.get_file_by_path("/$Secure")
            if not self.secure_file:
                raise Exception("Could not find $Secure file")
            
            self.sds_stream = self.secure_file.get_data_stream_by_name("$SDS")
            if not self.sds_stream:
                raise Exception("Could not find $SDS stream in $Secure")
            
            logger.info(f"$SDS stream size: {self.sds_stream.get_size():,} bytes")
            
        except Exception as e:
            logger.error(f"Error initializing $Secure analysis: {e}")
            raise
    
    def read_sds_entry(self, offset: int, max_size: int = 65536) -> Optional[EnhancedSDSEntry]:
        """Read and parse an enhanced SDS entry"""
        try:
            # Read initial header to get size
            header_data = self.sds_stream.read_segment(offset, 20)
            if len(header_data) < 20:
                return None
            
            # Parse header to determine total size needed
            hash_val, sec_id, sd_offset, sd_size, reserved = struct.unpack('<IIIII', header_data)
            
            # Validate entry
            if sd_size == 0 or sd_size > max_size:
                return None
            
            # Calculate total aligned size
            total_size = 20 + sd_size
            aligned_size = (total_size + 15) & ~15  # 16-byte alignment
            
            # Read full entry
            entry_data = self.sds_stream.read_segment(offset, aligned_size)
            if len(entry_data) < total_size:
                return None
            
            return EnhancedSDSEntry(entry_data[:total_size], offset)
            
        except Exception as e:
            logger.debug(f"Error reading SDS entry at offset 0x{offset:x}: {e}")
            return None
    
    def scan_sds_stream_parallel(self, start_offset: int = 0, max_entries: int = 1000, 
                                num_workers: int = 4) -> List[EnhancedSDSEntry]:
        """Parallel scanning of SDS stream for better performance"""
        entries = []
        stream_size = self.sds_stream.get_size()
        
        logger.info(f"Scanning $SDS stream from offset 0x{start_offset:x} with {num_workers} workers...")
        
        # Create offset ranges for parallel processing
        chunk_size = min(0x100000, (stream_size - start_offset) // num_workers)  # 1MB chunks
        offset_ranges = []
        
        current_offset = start_offset
        while current_offset < stream_size and len(entries) < max_entries:
            end_offset = min(current_offset + chunk_size, stream_size)
            offset_ranges.append((current_offset, end_offset))
            current_offset = end_offset
        
        # Process ranges in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
            future_to_range = {
                executor.submit(self._scan_offset_range, start, end, max_entries // len(offset_ranges)): (start, end)
                for start, end in offset_ranges
            }
            
            for future in concurrent.futures.as_completed(future_to_range):
                range_start, range_end = future_to_range[future]
                try:
                    range_entries = future.result()
                    entries.extend(range_entries)
                    if len(entries) >= max_entries:
                        break
                except Exception as e:
                    logger.warning(f"Error processing range 0x{range_start:x}-0x{range_end:x}: {e}")
        
        # Sort by offset and limit results
        entries.sort(key=lambda x: x.offset)
        return entries[:max_entries]
    
    def _scan_offset_range(self, start_offset: int, end_offset: int, max_entries: int) -> List[EnhancedSDSEntry]:
        """Scan a specific offset range for SDS entries"""
        entries = []
        offset = start_offset
        
        while offset < end_offset and len(entries) < max_entries:
            entry = self.read_sds_entry(offset)
            if entry:
                entries.append(entry)
                # Move to next aligned entry
                entry_size = 20 + entry.sd_size
                aligned_size = (entry_size + 15) & ~15
                offset += aligned_size
            else:
                # Try next 16-byte boundary
                offset = (offset + 16) & ~15
                # Don't search too far without finding entries
                if offset - start_offset > 0x10000:
                    break
        
        return entries
    
    def analyze_mft_security_references(self, max_entries: int = 1000) -> Dict[int, List[str]]:
        """Analyze security references in MFT entries"""
        security_usage = defaultdict(list)
        
        try:
            mft_file = self.fs.get_file_by_path("/$MFT")
            if not mft_file:
                logger.warning("Cannot access $MFT for security analysis")
                return security_usage
            
            logger.info("Analyzing MFT security references...")
            
            for i in range(min(max_entries, mft_file.get_number_of_entries())):
                try:
                    entry = mft_file.get_file_entry(i)
                    if entry and not entry.is_empty():
                        # Get security ID if available
                        security_id = getattr(entry, 'security_descriptor_identifier', None)
                        if security_id:
                            file_path = self._get_file_path(entry) or f"MFT[{i}]"
                            security_usage[security_id].append(file_path)
                except:
                    continue
            
            logger.info(f"Found {len(security_usage)} unique security IDs in use")
            
        except Exception as e:
            logger.error(f"Error analyzing MFT security references: {e}")
        
        return dict(security_usage)
    
    def _get_file_path(self, entry) -> Optional[str]:
        """Get full file path for MFT entry"""
        try:
            if hasattr(entry, 'get_name') and entry.get_name():
                return entry.get_name()
            # Could implement more sophisticated path reconstruction here
            return None
        except:
            return None
    
    def generate_comprehensive_report(self, output_format: str = 'json', 
                                   max_entries: int = 100) -> Dict[str, Any]:
        """Generate comprehensive security analysis report"""
        start_time = time.time()
        
        logger.info("Generating comprehensive security analysis report...")
        
        # Gather SDS entries
        sds_entries = self.scan_sds_stream_parallel(max_entries=max_entries)
        
        # Analyze MFT security usage
        security_usage = self.analyze_mft_security_references(max_entries=1000)
        
        # Build comprehensive report
        report = {
            'analysis_info': {
                'timestamp': datetime.now().isoformat(),
                'image_path': self.image_path,
                'analysis_duration_seconds': time.time() - start_time,
                'entries_analyzed': len(sds_entries)
            },
            'volume_info': asdict(self.volume_info),
            'security_summary': {
                'total_security_descriptors': len(sds_entries),
                'unique_security_ids': len(set(entry.security_id for entry in sds_entries)),
                'security_ids_in_use': len(security_usage),
                'hash_validation_passed': sum(1 for entry in sds_entries if entry.hash_valid),
                'hash_validation_failed': sum(1 for entry in sds_entries if not entry.hash_valid)
            },
            'sds_entries': [entry.to_dict() for entry in sds_entries],
            'security_usage': security_usage
        }
        
        logger.info(f"Report generated in {time.time() - start_time:.2f} seconds")
        return report
    
    def export_report(self, report: Dict[str, Any], output_path: str, format_type: str = 'json'):
        """Export report in various formats"""
        output_file = Path(output_path)
        
        if format_type.lower() == 'json':
            with open(output_file.with_suffix('.json'), 'w') as f:
                json.dump(report, f, indent=2, default=str)
                
        elif format_type.lower() == 'xml':
            root = self._dict_to_xml(report, 'ntfs_security_analysis')
            tree = ET.ElementTree(root)
            tree.write(output_file.with_suffix('.xml'), encoding='utf-8', xml_declaration=True)
            
        elif format_type.lower() == 'txt':
            with open(output_file.with_suffix('.txt'), 'w') as f:
                f.write(self._format_text_report(report))
        
        logger.info(f"Report exported to {output_file} in {format_type.upper()} format")
    
    def _dict_to_xml(self, data: Any, tag: str = 'item') -> ET.Element:
        """Convert dictionary to XML element"""
        elem = ET.Element(tag)
        
        if isinstance(data, dict):
            for key, value in data.items():
                child = self._dict_to_xml(value, str(key))
                elem.append(child)
        elif isinstance(data, list):
            for i, item in enumerate(data):
                child = self._dict_to_xml(item, f'item_{i}')
                elem.append(child)
        else:
            elem.text = str(data)
        
        return elem
    
    def _format_text_report(self, report: Dict[str, Any]) -> str:
        """Format report as human-readable text"""
        lines = []
        lines.append("NTFS Security Analysis Report")
        lines.append("=" * 50)
        lines.append("")
        
        # Analysis info
        info = report['analysis_info']
        lines.append(f"Analysis Date: {info['timestamp']}")
        lines.append(f"Image Path: {info['image_path']}")
        lines.append(f"Duration: {info['analysis_duration_seconds']:.2f} seconds")
        lines.append("")
        
        # Volume info
        vol = report['volume_info']
        lines.append("Volume Information:")
        lines.append(f"  Total Size: {vol.get('volume_size', 0):,} bytes")
        lines.append(f"  Cluster Size: {vol.get('cluster_size', 0):,} bytes")
        lines.append(f"  MFT Size: {vol.get('mft_size', 0):,} bytes")
        lines.append(f"  Serial Number: {vol.get('serial_number', 'Unknown')}")
        lines.append("")
        
        # Security summary
        summary = report['security_summary']
        lines.append("Security Summary:")
        lines.append(f"  Total Security Descriptors: {summary['total_security_descriptors']}")
        lines.append(f"  Unique Security IDs: {summary['unique_security_ids']}")
        lines.append(f"  Security IDs in Use: {summary['security_ids_in_use']}")
        lines.append(f"  Hash Validation Passed: {summary['hash_validation_passed']}")
        lines.append(f"  Hash Validation Failed: {summary['hash_validation_failed']}")
        lines.append("")
        
        # SDS entries
        lines.append("Security Descriptor Entries:")
        lines.append("-" * 30)
        for i, entry in enumerate(report['sds_entries'][:10]):  # Show first 10
            lines.append(f"\nEntry {i+1}:")
            lines.append(f"  Offset: {entry['offset']}")
            lines.append(f"  Security ID: {entry['security_id']}")
            lines.append(f"  Size: {entry['sd_size']} bytes")
            lines.append(f"  Hash Valid: {entry['hash']['valid']}")
            
            if entry['security_descriptor']:
                sd = entry['security_descriptor']
                lines.append(f"  Owner SID: {sd.get('owner_sid', 'None')}")
                lines.append(f"  Group SID: {sd.get('group_sid', 'None')}")
                if sd.get('dacl'):
                    lines.append(f"  DACL ACEs: {sd['dacl']['ace_count']}")
                if sd.get('sacl'):
                    lines.append(f"  SACL ACEs: {sd['sacl']['ace_count']}")
        
        if len(report['sds_entries']) > 10:
            lines.append(f"\n... and {len(report['sds_entries']) - 10} more entries")
        
        return "\n".join(lines)
    
    def close(self):
        """Clean up resources"""
        if self.fs:
            try:
                self.fs.close()
                logger.info("NTFS image closed")
            except:
                pass

class NTFSSecurityCLI:
    """Command-line interface for NTFS security analysis"""
    
    def __init__(self):
        self.analyzer = None
    
    def create_parser(self) -> argparse.ArgumentParser:
        """Create argument parser"""
        parser = argparse.ArgumentParser(
            description="Advanced NTFS Security Descriptor Analyzer",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
  # Basic analysis with 50 entries
  python ntfs_analyzer.py image.dd --count 50
  
  # Full analysis with JSON export
  python ntfs_analyzer.py image.dd --full-analysis --output report --format json
  
  # Extract specific Security ID
  python ntfs_analyzer.py image.dd --security-id 1234 --format raw-hex
  
  # Scan from specific offset
  python ntfs_analyzer.py image.dd --offset 0x1D8000 --count 10
  
  # Performance analysis with parallel processing
  python ntfs_analyzer.py image.dd --parallel --workers 8 --count 1000
            """
        )
        
        parser.add_argument("image", help="NTFS image file path")
        
        # Analysis options
        analysis_group = parser.add_argument_group("Analysis Options")
        analysis_group.add_argument("--offset", type=lambda x: int(x, 0), default=0,
                                  help="Starting offset in $SDS stream (hex or decimal)")
        analysis_group.add_argument("--count", type=int, default=100,
                                  help="Number of entries to analyze (default: 100)")
        analysis_group.add_argument("--security-id", type=int,
                                  help="Extract specific Security ID")
        analysis_group.add_argument("--full-analysis", action="store_true",
                                  help="Perform comprehensive analysis including MFT references")
        analysis_group.add_argument("--parallel", action="store_true",
                                  help="Use parallel processing for better performance")
        analysis_group.add_argument("--workers", type=int, default=4,
                                  help="Number of worker threads for parallel processing")
        
        # Output options
        output_group = parser.add_argument_group("Output Options")
        output_group.add_argument("--format", choices=["json", "xml", "txt", "raw-hex", "detailed"],
                                default="detailed", help="Output format")
        output_group.add_argument("--output", help="Output file path (without extension)")
        output_group.add_argument("--verbose", "-v", action="store_true",
                                help="Enable verbose logging")
        output_group.add_argument("--quiet", "-q", action="store_true",
                                help="Suppress non-error output")
        
        # Advanced options
        advanced_group = parser.add_argument_group("Advanced Options")
        advanced_group.add_argument("--validate-hashes", action="store_true",
                                  help="Validate security descriptor hashes")
        advanced_group.add_argument("--include-empty", action="store_true",
                                  help="Include empty or invalid entries")
        advanced_group.add_argument("--max-sd-size", type=int, default=65536,
                                  help="Maximum security descriptor size to process")
        
        return parser
    
    def setup_logging(self, verbose: bool, quiet: bool):
        """Configure logging based on arguments"""
        if quiet:
            logging.getLogger().setLevel(logging.ERROR)
        elif verbose:
            logging.getLogger().setLevel(logging.DEBUG)
        else:
            logging.getLogger().setLevel(logging.INFO)
    
    def run(self, args):
        """Main execution logic"""
        self.setup_logging(args.verbose, args.quiet)
        
        try:
            # Initialize analyzer
            self.analyzer = AdvancedNTFSAnalyzer(args.image)
            self.analyzer.open()
            
            if args.security_id:
                # Extract specific security ID
                self.extract_specific_security_id(args)
            elif args.full_analysis:
                # Comprehensive analysis
                self.perform_full_analysis(args)
            else:
                # Standard analysis
                self.perform_standard_analysis(args)
                
        except KeyboardInterrupt:
            logger.info("Analysis interrupted by user")
        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            return 1
        finally:
            if self.analyzer:
                self.analyzer.close()
        
        return 0
    
    def extract_specific_security_id(self, args):
        """Extract and display specific security ID"""
        logger.info(f"Searching for Security ID {args.security_id}...")
        
        # Scan for the specific security ID
        found_entry = None
        entries = self.analyzer.scan_sds_stream_parallel(
            start_offset=args.offset,
            max_entries=10000,  # Search more entries when looking for specific ID
            num_workers=args.workers if args.parallel else 1
        )
        
        for entry in entries:
            if entry.security_id == args.security_id:
                found_entry = entry
                break
        
        if not found_entry:
            logger.error(f"Security ID {args.security_id} not found")
            return
        
        # Output based on format
        if args.format == "raw-hex":
            if found_entry.security_descriptor:
                print(found_entry.security_descriptor.raw_data.hex())
            else:
                logger.error("No valid security descriptor found")
        elif args.format == "json":
            print(json.dumps(found_entry.to_dict(), indent=2))
        else:
            self.print_detailed_entry(found_entry)
    
    def perform_standard_analysis(self, args):
        """Perform standard SDS stream analysis"""
        logger.info(f"Analyzing {args.count} entries starting from offset 0x{args.offset:x}")
        
        if args.parallel:
            entries = self.analyzer.scan_sds_stream_parallel(
                start_offset=args.offset,
                max_entries=args.count,
                num_workers=args.workers
            )
        else:
            entries = self.analyzer.scan_sds_stream_parallel(
                start_offset=args.offset,
                max_entries=args.count,
                num_workers=1
            )
        
        if not entries:
            logger.warning("No valid entries found")
            return
        
        # Output results
        if args.format == "json":
            data = [entry.to_dict() for entry in entries]
            output = json.dumps(data, indent=2)
            if args.output:
                with open(f"{args.output}.json", 'w') as f:
                    f.write(output)
                logger.info(f"Results saved to {args.output}.json")
            else:
                print(output)
        
        elif args.format == "detailed":
            for i, entry in enumerate(entries):
                print(f"\n{'='*60}")
                print(f"Entry {i+1}/{len(entries)}")
                print('='*60)
                self.print_detailed_entry(entry)
        
        # Save to file if requested
        if args.output and args.format != "json":
            self.save_results(entries, args.output, args.format)
    
    def perform_full_analysis(self, args):
        """Perform comprehensive analysis"""
        logger.info("Performing comprehensive NTFS security analysis...")
        
        report = self.analyzer.generate_comprehensive_report(
            output_format=args.format,
            max_entries=args.count
        )
        
        if args.output:
            self.analyzer.export_report(report, args.output, args.format)
        else:
            if args.format == "json":
                print(json.dumps(report, indent=2, default=str))
            else:
                print(self.analyzer._format_text_report(report))
    
    def print_detailed_entry(self, entry: EnhancedSDSEntry):
        """Print detailed information about an SDS entry"""
        print(f"SDS Entry at offset {entry.offset}:")
        print(f"  Security ID: {entry.security_id}")
        print(f"  Hash: 0x{entry.hash:08x} ({'VALID' if entry.hash_valid else 'INVALID'})")
        print(f"  Size: {entry.sd_size} bytes")
        
        if entry.security_descriptor:
            sd = entry.security_descriptor
            print(f"  Security Descriptor:")
            print(f"    Revision: {sd.revision}")
            print(f"    Control Flags: {', '.join(sd.get_control_flags())}")
            print(f"    Owner SID: {sd.owner_sid.to_string() if sd.owner_sid else 'None'}")
            print(f"    Group SID: {sd.group_sid.to_string() if sd.group_sid else 'None'}")
            
            if sd.dacl:
                print(f"    DACL: {sd.dacl.ace_count} ACEs")
                for j, ace in enumerate(sd.dacl.aces[:3]):  # Show first 3 ACEs
                    print(f"      ACE {j+1}: {ace.get_type_string()}")
                    print(f"        SID: {ace.sid.to_string() if ace.sid else 'None'}")
                    print(f"        Access: 0x{ace.access_mask:08x}")
                if len(sd.dacl.aces) > 3:
                    print(f"      ... and {len(sd.dacl.aces) - 3} more ACEs")
            
            if sd.sacl:
                print(f"    SACL: {sd.sacl.ace_count} ACEs")
    
    def save_results(self, entries: List[EnhancedSDSEntry], output_path: str, format_type: str):
        """Save results to file"""
        if format_type == "xml":
            root = ET.Element("ntfs_security_entries")
            for entry in entries:
                entry_elem = self.analyzer._dict_to_xml(entry.to_dict(), "sds_entry")
                root.append(entry_elem)
            tree = ET.ElementTree(root)
            tree.write(f"{output_path}.xml", encoding='utf-8', xml_declaration=True)
            
        elif format_type == "txt":
            with open(f"{output_path}.txt", 'w') as f:
                for i, entry in enumerate(entries):
                    f.write(f"Entry {i+1}:\n")
                    f.write(f"{'='*40}\n")
                    # Write detailed entry info...
                    f.write(f"Security ID: {entry.security_id}\n")
                    f.write(f"Offset: {entry.offset}\n")
                    if entry.security_descriptor:
                        f.write(f"Raw hex: {entry.security_descriptor.raw_data.hex()}\n")
                    f.write("\n")
        
        logger.info(f"Results saved to {output_path}.{format_type}")

def main():
    """Main entry point"""
    cli = NTFSSecurityCLI()
    parser = cli.create_parser()
    args = parser.parse_args()
    
    return cli.run(args)

if __name__ == "__main__":
    sys.exit(main())